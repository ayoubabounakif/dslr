{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "  def __init__(self, learning_rate=0.01, max_iter=1000, threshold=0.5):\n",
    "    self.weights = None\n",
    "    self.bias = None\n",
    "    self.learning_rate = learning_rate\n",
    "    self.max_iter = max_iter\n",
    "    self.threshold = threshold\n",
    "\n",
    "  def loss(self, y, hypothesis):\n",
    "    m = len(y)\n",
    "    epsilon = 1e-5\n",
    "    j_theta = (-1 / m) * np.sum((y * np.log(hypothesis + epsilon)) + ((1 - y) * np.log(1 - hypothesis + epsilon)))\n",
    "    return j_theta\n",
    "  \n",
    "  def sigmoid(self, z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "  def hypothesis(self, X):\n",
    "    return self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    m, n = X.shape\n",
    "    self.weights = np.zeros(n)\n",
    "    self.bias = 0\n",
    "\n",
    "    for _ in range(self.max_iter):\n",
    "      # Forward Propagation\n",
    "      y_hat = self.hypothesis(X)\n",
    "\n",
    "      # Compute Average Loss\n",
    "      avg_loss = self.loss(y, y_hat)\n",
    "\n",
    "      # Backward Propagation\n",
    "      derivative_theta = (1 / m) * np.dot(y_hat - y, X)\n",
    "      derivative_bias = (1 / m) * np.sum(y_hat - y)\n",
    "      self.weights -= self.learning_rate * derivative_theta\n",
    "      self.bias -= self.learning_rate * derivative_bias\n",
    "\n",
    "  def predict(self, X):\n",
    "    return self.hypothesis(X) >= self.threshold\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "\n",
    "df = load_iris()\n",
    "X = df.data\n",
    "y = df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.7)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hogwarts Dataset\n",
    "\n",
    "df = pd.read_csv('../datasets/dataset_train.csv')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['target'] = label_encoder.fit_transform(df['Hogwarts House'])\n",
    "\n",
    "numeric_features = df.select_dtypes(include=[np.number])\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "numeric_features = pd.DataFrame(imp.fit_transform(numeric_features), columns=numeric_features.columns)\n",
    "\n",
    "X = numeric_features.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression(max_iter=1000)\n",
    "\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    }
   ],
   "source": [
    "MyLR = MyLogisticRegression(learning_rate=0.01, max_iter=1000, threshold=0.5)\n",
    "\n",
    "MyLR.fit(X_train, y_train)\n",
    "y_pred = MyLR.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "42AI-aybeee_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
